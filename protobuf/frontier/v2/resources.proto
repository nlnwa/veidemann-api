syntax = "proto3";

package veidemann.api.frontier.v2;

import "commons/v2/resources.proto";
import "config/v2/resources.proto";
import "google/protobuf/timestamp.proto";

option go_package = "github.com/nlnwa/veidemann-api/go/frontier/v2;frontier";
option java_multiple_files = true;
option java_package = "no.nb.nna.veidemann.api.frontier.v2";
option java_outer_classname = "FrontierResources";

message QueuedUrl {
    google.protobuf.Timestamp discovered_time_stamp = 3;
    // Sequence number to order the fetch of uris from a seed
    int64 sequence = 4;
    string uri = 5;
    string ip = 7;

    /**
     * Get the discoveryPath,
     *  R - Redirect
     *  E - Embed
     *  X - Speculative embed (aggressive/Javascript link extraction)
     *  L - Link
     *  P - Prerequisite (as for DNS or robots.txt before another URI)
     */
    string discovery_path = 8;
    string referrer = 9;
    repeated Cookie cookies = 10;
    int64 page_fetch_time_ms = 11; // The time used to fetch and render the the uri including dependencies
    int32 retries = 12; // Number of times this uri has been scheduled for retry.
    google.protobuf.Timestamp earliest_fetch_time_stamp = 13; // Do not fetch this uri before this time
    string crawl_host_group_id = 14; // The Crawl Host Group calculated for this uri
    veidemann.api.config.v2.ConfigRef politeness_ref = 15; // Ref to the politeness config used when discovering this uri
    veidemann.api.commons.v2.Error error = 16; // Contains the error reason if fetch failed
    bool unresolved = 18; // If true, then this uri is just added to the queue and no resolution of ip or robots.txt checks are done yet.
    google.protobuf.Timestamp fetch_start_time_stamp = 19;
    // The weighting between jobs when two jobs compete on fetching resources from the same hosts.
    // Copied from CrawlConfig for efficiency.
    double priority_weight = 20;
    // The seed uri which was the starting point for this uri.
    string seed_uri = 21;
    // Annotations used as parameters to scripts.
    repeated veidemann.api.config.v2.Annotation annotation = 22;
}

message Cookie {
    // Cookie name.
    string name = 1;
    // Cookie value.
    string value = 2;
    // Cookie domain.
    string domain = 3;
    // Cookie path.
    string path = 4;
    // Cookie expiration date as the number of seconds since the UNIX epoch.
    double expires = 5;
    // Cookie size.
    int32 size = 6;
    // True if cookie is http-only.
    bool http_only = 7;
    // True if cookie is secure.
    bool secure = 8;
    // True in case of session cookie.
    bool session = 9;
    // Cookie SameSite type.
    string same_site = 10;
}

message CrawlHostGroup {
    // If IP-address is not resolved this is SHA-1 of URL.
    // After IP-resolution this is SHA-1 hash of IP or CrawlHostGroupConfig id if one such config matched.
    string id = 1;
    // Lover limit for time between pageloads from this CrawlHostGroup.
    int64 min_time_between_page_load_ms = 2;
    // Upper limit for time between pageloads from this CrawlHostGroup.
    // This is the upper limit for calculation of dealy time, but actual time might be higher depending on
    // the harvesters capacity.
    int64 max_time_between_page_load_ms = 3;
    // The fetch time of the URI is multiplied with this value to get the delay time before fetching the next URI.
    // If min_time_between_page_load_ms and/or max_time_between_page_load_ms are set, then those values are used as
    // the upper/lower limits for delay.
    // If delay_factor is unset or zero, then a delay_facor of one is assumed. If delay_factor is negative,
    // a delay_factor of zero is assumed.
    float delay_factor = 4;
    // The maximum number of retries before giving up fetching a uri.
    int32 max_retries = 5;
    // The minimum time before a failed page load is retried.
    int32 retry_delay_seconds = 6;
    // The number of queued Uri's belonging to this CrawlHostGroup
    int64 queued_uri_count = 7;
    // If this CrawlHostGroup is busy, this field contains the id of the uri currently beeing fetched.
    string current_uri_id = 8;
    // Token to guard against two harvesters responding to the same request.
    string session_token = 9;
    // The time when frontier sent a PageHarvestSpec to a harvester.
    google.protobuf.Timestamp fetch_start_time_stamp = 10;
}

// Metadata about a crawl execution.
// A crawl execution is the complete harvest of a seed as specified in the connected job's configuration.
message CrawlExecutionStatus {
    enum State {
        UNDEFINED = 0;
        CREATED = 1;
        FETCHING = 2;
        SLEEPING = 3;
        FINISHED = 4;
        ABORTED_TIMEOUT = 5;
        ABORTED_SIZE = 6;
        ABORTED_MANUAL = 7;
        FAILED = 8;
        DIED = 9;
    }

    string id = 1;
    State state = 2;
    string job_id = 3;
    string seed_id = 4;
    google.protobuf.Timestamp start_time = 6; // When this crawl execution started crawling
    google.protobuf.Timestamp end_time = 7; // When this crawl execution ended
    int64 documents_crawled = 8;
    int64 bytes_crawled = 9;
    int64 uris_crawled = 10;
    int64 documents_failed = 11;
    int64 documents_out_of_scope = 12;
    int64 documents_retried = 13;
    int64 documents_denied = 14;
    google.protobuf.Timestamp last_change_time = 15; // When this record was last updated
    google.protobuf.Timestamp created_time = 16; // When this crawl execution was created
    repeated string current_uri_id = 20;
    string job_execution_id = 21;
    veidemann.api.commons.v2.Error error = 22; // Extra description of error state
    State desired_state = 23; // Used when external process want to abort execution
}

message CrawlExecutionStatusChange {
    string id = 1;
    CrawlExecutionStatus.State state = 2;
    google.protobuf.Timestamp end_time = 4; // When this crawl execution ended
    int64 add_documents_crawled = 5;
    int64 add_bytes_crawled = 6;
    int64 add_uris_crawled = 7;
    int64 add_documents_failed = 8;
    int64 add_documents_out_of_scope = 9;
    int64 add_documents_retried = 10;
    int64 add_documents_denied = 11;
    QueuedUrl add_current_uri = 12;
    QueuedUrl delete_current_uri = 13;
    veidemann.api.commons.v2.Error error = 14; // Extra description of error state
}

// Metadata about an execution of a job.
// A job execution is the sum of all crawl executions for a job at a specific time.
message JobExecutionStatus {
    enum State {
        UNDEFINED = 0;
        CREATED = 1;
        RUNNING = 2;
        FINISHED = 3;
        ABORTED_MANUAL = 4;
        FAILED = 5;
        DIED = 6;
    }

    string id = 1;
    string job_id = 2;
    State state = 3;
    map<string, int32> executions_state = 4;
    google.protobuf.Timestamp start_time = 6;
    google.protobuf.Timestamp end_time = 7;
    int64 documents_crawled = 8;
    int64 bytes_crawled = 9;
    int64 uris_crawled = 10;
    int64 documents_failed = 11;
    int64 documents_out_of_scope = 12;
    int64 documents_retried = 13;
    int64 documents_denied = 14;
    veidemann.api.commons.v2.Error error = 15; // Extra description of error state
    State desired_state = 16; // Used when external process want to abort execution
}
